{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14aefa4a-8056-44ce-a992-8b3dd8167d22",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9bf18c51-a92f-4c1d-8649-20c0e0de33cf",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cv2'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m layers, Model, Input, backend \u001b[38;5;28;01mas\u001b[39;00m K\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptimizers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Adam\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcv2\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcsv\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'cv2'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model, Input, backend as K\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import cv2\n",
    "import csv\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637a1f41-c257-4812-af07-569c304e2f25",
   "metadata": {},
   "source": [
    "Enable GPU boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97419a0-deb5-4e01-968b-83d9bce79de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(\"GPU boost set.\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddcab6f3-d6b9-4dcd-892f-79b9f36ed82e",
   "metadata": {},
   "source": [
    "Set paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d555da2d-f676-45e2-9fa5-8276c186c20b",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_DIR = \"/home/emizu/Desktop/SiamezeDataset\" \n",
    "MODEL_FILE = \"siamese_model.keras\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2e79aa-351a-414a-a382-9f08005dd390",
   "metadata": {},
   "source": [
    "Set images resizeing(and thus nominal) sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6b6fc3-df35-4829-86a3-e73a642a4b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_HEIGHT = 105\n",
    "IMG_WIDTH = 105\n",
    "IMG_CHANNELS = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e57787-1ec4-4581-89c6-8faa1c956599",
   "metadata": {},
   "source": [
    "Function to load the dataset into a dictionary with person names(strings) as keys and image lists as values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916c3849-303e-4fc1-9586-49de0972e8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset():\n",
    "    dataset = {}\n",
    "    for person in os.listdir(DATASET_DIR):\n",
    "        person_path = os.path.join(DATASET_DIR, person)\n",
    "        if os.path.isdir(person_path):\n",
    "            images = load_images_from_folder(person_path)\n",
    "            if len(images) > 1:\n",
    "                dataset[person] = images\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcdc6056-e974-43ac-9eb6-99d647b38258",
   "metadata": {},
   "source": [
    "Function to obtain all images of a person. It is meant to resize them, normalize them, group them into a list and return that list. An image is a tuple of the image label and actual image data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "850cdc88-67a9-4386-ab5c-1191ac5c2e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images_from_folder(folder):\n",
    "    images = []\n",
    "    for filename in os.listdir(folder):\n",
    "        if filename.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "            img_path = os.path.join(folder, filename)\n",
    "            img = cv2.imread(img_path)\n",
    "            if img is not None:\n",
    "                # Resize image\n",
    "                img = cv2.resize(img, (IMG_WIDTH, IMG_HEIGHT))\n",
    "                # Normalize image to [0, 1]\n",
    "                img = img.astype(\"float32\") / 255.0\n",
    "                images.append((filename, img))\n",
    "    return images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a790f4-86d5-49da-a465-5a8b088b2981",
   "metadata": {},
   "source": [
    "Function to split the dataset. Will return 2 subtatasets in the form of 2 dictionaries, one for training and one for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1d99a8-cc6d-4790-90b3-bc4904e7d711",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(dataset):\n",
    "    train_set, val_set = {}, {}\n",
    "    for person, images in dataset.items():\n",
    "        random.shuffle(images)\n",
    "        n = len(images)\n",
    "        train_end = int(0.6 * n)\n",
    "        val_end = train_end + int(0.2 * n)\n",
    "        train_set[person] = images[:train_end]\n",
    "        val_set[person] = images[train_end:val_end]\n",
    "    return train_set, val_set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe721e23-73b8-45b6-8f18-34c760cc47d7",
   "metadata": {},
   "source": [
    "Function to create pairs of images(tuples) from a dataset. Each pair will contain 2 images, a label(1 for pozitive pairs and 0 for negative pairs) and a metadata about the images(which is not fed into the model under any circumstance). The metadata is ment to serve as aditional informations when the output file is generated and for debuging purposes.\n",
    "A pozitive pair is a pair containing both images from the same person.\n",
    "A negative pair is a pair which contains 2 images of 2 different persons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e5409d-0e4c-492c-92e3-04a4fdf173bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_pairs(data_dict):\n",
    "    positive_pairs = []\n",
    "    for person in data_dict:\n",
    "        images = data_dict[person]\n",
    "        for i in range(len(images)):\n",
    "            for j in range(i + 1, len(images)):\n",
    "                positive_pairs.append((images[i][1],images[j][1],1,(person, person, images[i][0], images[j][0])))\n",
    "\n",
    "    num_positive_pairs = len(positive_pairs)\n",
    "    negative_pairs = []\n",
    "    persons = list(data_dict.keys())\n",
    "\n",
    "    while len(negative_pairs) < num_positive_pairs:\n",
    "        person1 = random.choice(persons)\n",
    "        if not data_dict[person1]:\n",
    "            continue\n",
    "        fname1, img1 = random.choice(data_dict[person1])\n",
    "\n",
    "        other_persons = [p for p in persons if p != person1]\n",
    "        if not other_persons:\n",
    "            break  \n",
    "        person2 = random.choice(other_persons)\n",
    "        if not data_dict[person2]:\n",
    "            continue\n",
    "        fname2, img2 = random.choice(data_dict[person2])\n",
    "\n",
    "        negative_pairs.append((img1,img2,0,(person1, person2, fname1, fname2)))\n",
    "\n",
    "    all_pairs = positive_pairs + negative_pairs\n",
    "    random.shuffle(all_pairs)\n",
    "    return all_pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3973126-ea73-446f-91ba-60905ec1fc8c",
   "metadata": {},
   "source": [
    "Function to build the base neural network for the siamese network. It consists of convolutional layers for image processing and a dense layer for outputing the feature vector of an imput(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "991a71e1-db88-461e-b2fc-7b0ba716361f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_base_network(input_shape):\n",
    "    inp = Input(shape=input_shape)\n",
    "    x = layers.Conv2D(100, (10, 10), activation='relu')(inp)\n",
    "    x = layers.Conv2D(125, (10, 10), activation='relu')(x)\n",
    "    x = layers.MaxPooling2D()(x)\n",
    "    x = layers.Conv2D(150, (7, 7), activation='relu')(x)\n",
    "    x = layers.MaxPooling2D()(x)\n",
    "    x = layers.Conv2D(175, (4, 4), activation='relu')(x)\n",
    "    x = layers.Conv2D(250, (4, 4), activation='relu')(x)\n",
    "    x = layers.MaxPooling2D()(x)\n",
    "    x = layers.Conv2D(250, (2, 2), activation='relu')(x)\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(5000, activation='sigmoid')(x)\n",
    "    return Model(inp, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597267b4-aaa0-4d10-ae91-1c23d9a66c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "Logical layer of the siamese network to compare the 2 inputs(their feature vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c6324c-2fa1-4ce7-9155-a298d42b8bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance(vects):\n",
    "    x, y = vects\n",
    "    sum_square = K.sum(K.square(x - y), axis=1, keepdims=True)\n",
    "    return K.sqrt(K.maximum(sum_square, K.epsilon()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c98f52da-ece0-4b36-980d-1a0151c1f2c4",
   "metadata": {},
   "source": [
    "Function to create the entire siamese neural network. On top of the base network and logical layer(lambda layer), another dense layer is added for deciding the final output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec22346-9dab-47f4-9353-e42faffca971",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_siamese_model(input_shape):\n",
    "    base_network = build_base_network(input_shape)\n",
    "\n",
    "    input_a = Input(shape=input_shape)\n",
    "    input_b = Input(shape=input_shape)\n",
    "\n",
    "    processed_a = base_network(input_a)\n",
    "    processed_b = base_network(input_b)\n",
    "\n",
    "    distance = layers.Lambda(euclidean_distance)([processed_a, processed_b])\n",
    "    outputs = layers.Dense(1, activation='sigmoid')(distance)\n",
    "\n",
    "    model = Model([input_a, input_b], outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b4b33d6-14a2-4973-a142-7eeece8b079c",
   "metadata": {},
   "source": [
    "Function to generate batches from pairs. This function decides what elements from a pair are fed into the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097aca1f-676b-465a-b18a-6cc166677826",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch(pairs, batch_size=32):\n",
    "    while True:\n",
    "        random.shuffle(pairs)\n",
    "        for i in range(0, len(pairs), batch_size):\n",
    "            batch = pairs[i:i+batch_size]\n",
    "            imgs_a = np.array([pair[0] for pair in batch])\n",
    "            imgs_b = np.array([pair[1] for pair in batch])\n",
    "            labels = np.array([pair[2] for pair in batch])\n",
    "            yield (imgs_a, imgs_b), labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b620940-fc32-4f02-97b6-911ab377a42e",
   "metadata": {},
   "source": [
    "Load and split dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ddfeabc-e8b5-4443-9627-a18644436ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading and preparing dataset...\")\n",
    "dataset = prepare_dataset()\n",
    "if not dataset:\n",
    "    print(\"Dataset not found or no valid sub-folders/images. Exiting.\")\n",
    "    sys.exit(1)\n",
    "train_set, val_set = split_dataset(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b1e0c6-ac21-4181-b710-ec71afdd5864",
   "metadata": {},
   "source": [
    "Create pairs for training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ce8d2c-ae92-4b5b-bfdd-303612db078f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Creating pairs for training and validation...\")\n",
    "train_pairs = make_pairs(train_set)\n",
    "val_pairs = make_pairs(val_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769ccd96-ba50-4833-942e-19a616203259",
   "metadata": {},
   "source": [
    "Create final training and validation dataset variants with output signatures as required by tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dec88db-0f97-4db3-b7e9-eaa5087b8fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_generator(\n",
    "    lambda: generate_batch(train_pairs, batch_size),\n",
    "    output_signature=(\n",
    "        (\n",
    "            tf.TensorSpec(shape=(None, IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS), dtype=tf.float32),\n",
    "            tf.TensorSpec(shape=(None, IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS), dtype=tf.float32)\n",
    "        ),\n",
    "        tf.TensorSpec(shape=(None,), dtype=tf.float32)\n",
    "    )\n",
    ")\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_generator(\n",
    "    lambda: generate_batch(val_pairs, batch_size),\n",
    "    output_signature=(\n",
    "        (\n",
    "            tf.TensorSpec(shape=(None, IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS), dtype=tf.float32),\n",
    "            tf.TensorSpec(shape=(None, IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS), dtype=tf.float32)\n",
    "        ),\n",
    "        tf.TensorSpec(shape=(None,), dtype=tf.float32)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224ab172-265a-4185-9e7a-0689e54f5193",
   "metadata": {},
   "source": [
    "Get input shape from one image sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272ba7e2-bb82-42d2-96c3-286a02e4cb5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = next(iter(train_set.values()))[0][1]\n",
    "input_shape = sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87fee742-ae60-4bd9-a93c-216539aa7f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f51401-5232-4d21-aad7-e9b08872b4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Building new Siamese model...\")\n",
    "model = build_siamese_model(input_shape)\n",
    "optimizer = Adam(learning_rate=0.0001)\n",
    "model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2bec0d-b93a-49ec-b2c1-6facb9f28db4",
   "metadata": {},
   "source": [
    "Set training parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e269f4-eedc-4205-8afa-c3488a40888f",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 50\n",
    "epochs = 15\n",
    "steps_per_epoch = 1000\n",
    "validation_steps = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df064fcd-8b81-4822-8664-d75ae8f3875f",
   "metadata": {},
   "source": [
    "Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcaad911-8a5c-4817-9bda-20333cf16c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting training...\")\n",
    "model.fit(\n",
    "    train_dataset,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    epochs=epochs,\n",
    "    validation_data=val_dataset,\n",
    "    validation_steps=validation_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5861d6-78a7-4e7b-8567-35bd5dae0853",
   "metadata": {},
   "source": [
    "Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f55312-6d2f-4c2f-bc76-d81c1cddf4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training completed. Saving model...\")\n",
    "model.save(MODEL_FILE)\n",
    "print(f\"Model saved to {MODEL_FILE}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
